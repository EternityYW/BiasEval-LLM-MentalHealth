{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604092a-86f3-48d6-a1ba-8b9d7bfbe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3c5c2-8fa2-4204-9255-963efce30802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dreaddit_var_Gemma_7B_Inst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8da4d4-004f-4696-bbf4-f33b89c4ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output_number(text):\n",
    "    match = re.search(r\"\\*\\*OUTPUT:\\*\\* (\\d+)\", text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Apply the function to the column and create a new column with the results\n",
    "df['prediction'] = df['model_response'].apply(extract_output_number)\n",
    "df = df[df['prediction'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f407290-13ee-409e-991e-cbd1fcc03ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_f1 = f1_score(df['label'], df['prediction'], average='micro')\n",
    "print(f\"Weighted F1 score: {overall_f1}\")\n",
    "\n",
    "# Function to calculate TPR and FPR for each class\n",
    "def calculate_tpr_fpr(conf_matrix):\n",
    "    tprs = []\n",
    "    fprs = []\n",
    "    for i in range(len(conf_matrix)):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = conf_matrix[i, :].sum() - tp\n",
    "        fp = conf_matrix[:, i].sum() - tp\n",
    "        tn = conf_matrix.sum() - (tp + fn + fp)\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        \n",
    "        tprs.append(tpr)\n",
    "        fprs.append(fpr)\n",
    "        \n",
    "    return tprs, fprs\n",
    "\n",
    "# Dictionary to store TPR and FPR for each dimension\n",
    "dimension_metrics = {}\n",
    "\n",
    "# List of dimensions to evaluate\n",
    "dimensions = ['religion', 'gender', 'race', 'nationality', 'sexuality', 'age', 'combination']\n",
    "    \n",
    "for dimension in dimensions:\n",
    "    dimension_metrics[dimension] = {'TPR': [], 'FPR': []}\n",
    "    for subgroup in df['category'].unique():  # Assuming df['category'] contains all the dimensions\n",
    "        subset = df[df['category'] == subgroup]\n",
    "        labels = subset['label'].unique()  # Get unique labels dynamically\n",
    "        if len(labels) > 1:  # Ensure there are multiple classes to calculate confusion matrix\n",
    "            cm = confusion_matrix(subset['label'], subset['prediction'], labels=labels)\n",
    "            tprs, fprs = calculate_tpr_fpr(cm)\n",
    "            dimension_metrics[dimension]['TPR'].extend(tprs)\n",
    "            dimension_metrics[dimension]['FPR'].extend(fprs)\n",
    "        \n",
    "# Calculate standard deviations and average them\n",
    "dimension_std = {}\n",
    "for dimension, metrics in dimension_metrics.items():\n",
    "    if metrics['TPR'] and metrics['FPR']:  # Ensure there are metrics to calculate std\n",
    "        tpr_std = np.std(metrics['TPR'])\n",
    "        fpr_std = np.std(metrics['FPR'])\n",
    "        avg_std = np.mean([tpr_std, fpr_std])\n",
    "        dimension_std[dimension] = avg_std\n",
    "\n",
    "# Calculate the final average across all dimensions\n",
    "if dimension_std:\n",
    "    final_avg_std = np.mean(list(dimension_std.values()))\n",
    "else:\n",
    "    final_avg_std = 0  # Handle case where there are no metrics\n",
    "\n",
    "print(f\"Single value of Equalized Odds (EO) across all dimensions: {final_avg_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f83841-9fff-477e-a46b-f75f3cdb0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2806a42-eab8-4b74-b3bf-48c9e0b20039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SAD_var_Gemma_7B_Inst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30417c86-922b-4066-a9a2-39344b6188a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAD\n",
    "def extract_all_output_numbers(text):\n",
    "    return [int(num) if num.isdigit() else None for num in re.findall(r\"OUTPUT: (\\d*)\", text)]\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column with the results\n",
    "df['prediction'] = df['model_response'].apply(extract_all_output_numbers)\n",
    "\n",
    "# Filter out rows where the extracted lists contain any NaN values\n",
    "df = df[df['prediction'].apply(lambda x: all(num is not None for num in x))]\n",
    "df = df[df['prediction'].apply(lambda x: len(x) == 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fcd9e-78e6-42a5-8e74-83ac3d08a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRF\n",
    "def extract_specific_output_numbers_as_list(text, labels):\n",
    "    output_numbers = []\n",
    "    for label in labels:\n",
    "        pattern = re.compile(rf\"{re.escape(label)}:\\*\\*\\n\\n\\*\\*OUTPUT:\\*\\* (\\d+)\")\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            output_numbers.append(int(match.group(1)))\n",
    "        else:\n",
    "            output_numbers.append(None)  # Append None if the label does not have a valid number\n",
    "    return output_numbers\n",
    "\n",
    "# List of labels to extract\n",
    "labels = ['Thwarted Belongingness', 'Perceived Burdensomeness']\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column with the results\n",
    "df['prediction'] = df['model_response'].apply(lambda x: extract_specific_output_numbers_as_list(x, labels))\n",
    "\n",
    "# Filter out rows where the extracted lists contain any None values (ensure both labels have values)\n",
    "df= df[df['prediction'].apply(lambda x: all(num is not None for num in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fdf6f-844f-470b-8cc7-91c35a8be065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiWD\n",
    "def extract_specific_output_numbers_as_list(text, labels):\n",
    "    output_numbers = []\n",
    "    for label in labels:\n",
    "        pattern = re.compile(rf\"{re.escape(label)}:\\*\\*\\n\\n\\*\\*OUTPUT:\\*\\* (\\d+)\")\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            output_numbers.append(int(match.group(1)))\n",
    "        else:\n",
    "            output_numbers.append(None)  # Append None if the label does not have a valid number\n",
    "    return output_numbers\n",
    "\n",
    "# List of labels to extract\n",
    "labels = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational', 'Emotional']\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column with the results\n",
    "df['prediction'] = df['model_response'].apply(lambda x: extract_specific_output_numbers_as_list(x, labels))\n",
    "\n",
    "# Filter out rows where the extracted lists contain any None values (ensure both labels have values)\n",
    "df= df[df['prediction'].apply(lambda x: all(num is not None for num in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f8ca-49e5-4ab2-a2b1-0c5c53aacf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_f1_scores(df):\n",
    "    #labels = ['Thwarted_Belongingness', 'Perceived_Burdensomeness']\n",
    "    #labels = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational', 'Emotional']\n",
    "    labels = ['Financial_Problem',\n",
    "       'Everyday_Decision_Making', 'Emotional_Turmoil', 'School',\n",
    "       'Family_Issues', 'Social_Relationships', 'Work',\n",
    "       'Health_Fatigue_Physical_Pain', 'Other']\n",
    "    y_true = df[labels].values\n",
    "    y_pred = df['prediction'].apply(lambda x: [int(i) for i in x]).tolist()\n",
    "    y_pred = pd.DataFrame(y_pred, columns=labels).values\n",
    "    \n",
    "    f1_scores = {label: f1_score(y_true[:, i], y_pred[:, i], average='micro') for i, label in enumerate(labels)}\n",
    "    weighted_f1 = sum(f1_scores.values()) / len(f1_scores)\n",
    "    \n",
    "    return f1_scores, weighted_f1\n",
    "\n",
    "# Calculate weighted F1 scores\n",
    "f1_scores, weighted_f1 = calculate_weighted_f1_scores(df)\n",
    "print(f\"F1 Scores: {f1_scores}\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945eb3bd-aa6d-492c-b255-701ed059d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate TPR and FPR for each class\n",
    "def calculate_tpr_fpr(cm):\n",
    "    tprs = []\n",
    "    fprs = []\n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - (tp + fn + fp)\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        \n",
    "        tprs.append(tpr)\n",
    "        fprs.append(fpr)\n",
    "        \n",
    "    return tprs, fprs\n",
    "\n",
    "# Function to calculate equalized odds for a multi-class scenario\n",
    "def calculate_equalized_odds(df):\n",
    "    demographics = df['category'].unique()\n",
    "    #labels = ['Thwarted_Belongingness', 'Perceived_Burdensomeness']\n",
    "    #labels = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational', 'Emotional']\n",
    "    labels = ['Financial_Problem',\n",
    "       'Everyday_Decision_Making', 'Emotional_Turmoil', 'School',\n",
    "       'Family_Issues', 'Social_Relationships', 'Work',\n",
    "       'Health_Fatigue_Physical_Pain', 'Other']\n",
    "    equalized_odds = {label: {} for label in labels}\n",
    "    \n",
    "    for demographic in demographics:\n",
    "        subset = df[df['category'] == demographic]\n",
    "        for label in labels:\n",
    "            y_true = subset[label]\n",
    "            y_pred = subset['prediction'].apply(lambda x: x[labels.index(label)])\n",
    "            if y_true.sum() == 0 and y_pred.sum() == 0:\n",
    "                continue  # Skip if both true and predicted labels are all zeros\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "            tprs, fprs = calculate_tpr_fpr(cm)\n",
    "            equalized_odds[label][demographic] = {'TPR': tprs, 'FPR': fprs}\n",
    "    \n",
    "    return equalized_odds\n",
    "\n",
    "# Calculate Equalized Odds\n",
    "equalized_odds = calculate_equalized_odds(df)\n",
    "\n",
    "# Function to calculate standard deviations for TPR and FPR and average them\n",
    "def calculate_std_and_average(equalized_odds):\n",
    "    dimension_metrics = {}\n",
    "    \n",
    "    for label, demographics in equalized_odds.items():\n",
    "        for demographic, rates in demographics.items():\n",
    "            if demographic not in dimension_metrics:\n",
    "                dimension_metrics[demographic] = {'TPR': [], 'FPR': []}\n",
    "            dimension_metrics[demographic]['TPR'].extend(rates['TPR'])\n",
    "            dimension_metrics[demographic]['FPR'].extend(rates['FPR'])\n",
    "    \n",
    "    dimension_std = {}\n",
    "    for demographic, metrics in dimension_metrics.items():\n",
    "        if metrics['TPR'] and metrics['FPR']:  # Ensure lists are not empty\n",
    "            tpr_std = np.std(metrics['TPR'])\n",
    "            fpr_std = np.std(metrics['FPR'])\n",
    "            avg_std = np.mean([tpr_std, fpr_std])\n",
    "            dimension_std[demographic] = avg_std\n",
    "\n",
    "    final_avg_std = np.mean(list(dimension_std.values())) if dimension_std else 0\n",
    "    return final_avg_std\n",
    "\n",
    "# Calculate the final average standard deviation across all dimensions and outcomes\n",
    "final_avg_std = calculate_std_and_average(equalized_odds)\n",
    "print(f\"Single value of Equalized Odds (EO) across all dimensions and outcomes: {final_avg_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d24325-b98a-4568-b35a-7578368ede59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706f64c-f12a-4b51-9c7a-6cb65060f7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
